#+TITLE: Understanding Autoware NDT Scan Matching
#+AUTHOR: CUDA NDT Matcher Project
#+DATE: 2026-01-12
#+SETUPFILE: github.theme

* Introduction

This tutorial explains how Autoware's NDT (Normal Distributions Transform) scan matcher works. NDT is a probabilistic point cloud registration algorithm used for localization in autonomous driving.

The Autoware implementation is based on *Magnusson's 2009 PhD thesis*: "The Three-Dimensional Normal-Distributions Transform -- an Efficient Representation for Registration, Surface Analysis, and Loop Detection."

** What Does NDT Do?

NDT estimates the vehicle's pose (position + orientation) by matching a LiDAR scan against a pre-built 3D point cloud map. Given:

- *Map*: A 3D point cloud of the environment (stored as a voxel grid)
- *Scan*: Current LiDAR point cloud from the vehicle's sensors
- *Initial guess*: Approximate pose from odometry/IMU

NDT finds the optimal transformation that best aligns the scan to the map.

** Why NDT Over ICP?

| Feature        | ICP (Iterative Closest Point) | NDT                              |
|----------------+-------------------------------+----------------------------------|
| Representation | Raw points                    | Gaussian distributions per voxel |
| Smoothness     | Discrete, non-differentiable  | Smooth, differentiable score     |
| Speed          | O(N log M) per iteration      | O(N) per iteration               |
| Robustness     | Sensitive to outliers         | Inherently robust                |

NDT models the map as a collection of Gaussian distributions, providing a smooth cost function suitable for Newton optimization.

* Algorithm Overview

#+begin_src mermaid
flowchart TD
    A[Input: Scan + Initial Guess] --> B[Transform Scan Points]
    B --> C[Find Nearby Voxels]
    C --> D[Compute Score, Gradient, Hessian]
    D --> E[Newton Step: delta = -H^-1 * g]
    E --> F[Line Search]
    F --> G{Converged?}
    G -->|No| B
    G -->|Yes| H[Output: Final Pose]
#+end_src

The algorithm iterates until convergence:
1. Transform source points using current pose estimate
2. For each transformed point, find nearby voxels in the map
3. Compute the NDT score (negative log-likelihood) and its derivatives
4. Solve Newton step to get pose update direction
5. Apply line search to find optimal step length
6. Update pose and check convergence

* Voxel Grid Construction

The map is preprocessed into a voxel grid where each voxel stores a Gaussian distribution.

** Voxel Structure

Each voxel contains:

| Field        | Type      | Description                      |
|--------------+-----------+----------------------------------|
| =mean_=      | Vector3   | Centroid of points in voxel      |
| =cov_=       | Matrix3x3 | Covariance matrix                |
| =icov_=      | Matrix3x3 | Inverse covariance (precomputed) |
| =nr_points_= | int       | Number of points in voxel        |

** Building the Voxel Grid

#+begin_src cpp
// Pseudocode for voxel grid construction
void build_voxel_grid(PointCloud& map, float resolution) {
    // 1. Assign points to voxels
    for (auto& point : map) {
        int voxel_id = floor(point / resolution);
        voxels[voxel_id].points.push_back(point);
    }

    // 2. Compute statistics for each voxel
    for (auto& [id, voxel] : voxels) {
        if (voxel.points.size() < min_points_per_voxel)
            continue;  // Skip sparse voxels

        // Compute mean
        voxel.mean = sum(voxel.points) / voxel.points.size();

        // Compute covariance
        for (auto& p : voxel.points) {
            Vector3 d = p - voxel.mean;
            voxel.cov += d * d.transpose();
        }
        voxel.cov /= (voxel.points.size() - 1);

        // Regularize and invert covariance
        regularize(voxel.cov);  // Ensure positive definite
        voxel.icov = voxel.cov.inverse();
    }

    // 3. Build KD-tree on voxel centroids for fast lookup
    kdtree.build(voxel_centroids);
}
#+end_src

** Covariance Regularization

To ensure numerical stability, eigenvalue decomposition is used:

$$\Sigma = V \Lambda V^T$$

Small eigenvalues are clamped: $\lambda_i = \max(\lambda_i, 0.01 \cdot \lambda_{max})$

This prevents singular covariance matrices while preserving the principal directions.

#+begin_NOTE
*Performance Impact*: Voxel grid construction is expensive but done only once per map load. The KD-tree enables fast $O(\log V)$ voxel lookup during alignment.
#+end_NOTE

* NDT Score Function

The NDT score measures how well the scan aligns with the map. It's based on the probability of a point belonging to a voxel's Gaussian distribution.

** Probability Density

For a point $\mathbf{x}$ and voxel with mean $\boldsymbol{\mu}$ and covariance $\Sigma$:

$$p(\mathbf{x}) = -d_1 \cdot \exp\left(-\frac{d_2}{2} (\mathbf{x} - \boldsymbol{\mu})^T \Sigma^{-1} (\mathbf{x} - \boldsymbol{\mu})\right)$$

Where:
- $d_1, d_2$ are constants derived from the outlier ratio (typically 0.55)
- The negative sign makes this a cost to minimize

** Gaussian Fitting Constants

The constants $d_1$, $d_2$, $d_3$ are computed from the outlier ratio $p$:

#+begin_src cpp
// From multigrid_ndt_omp_impl.hpp lines 236-243
double p = outlier_ratio_;  // typically 0.55
gauss_d1_ = -log(p);
gauss_d2_ = -2 * log((-log(p * gauss_c2_)) / (-log(p)));
gauss_d3_ = gauss_d1_ + gauss_d2_ / 2.0;
#+end_src

These constants control the shape of the score function:
- Higher $d_1$ increases penalty for misalignment
- Higher $d_2$ sharpens the Gaussian peak

** Total Score

The total score is the sum over all source points and their nearby voxels:

$$S = \sum_{i=1}^{N} \sum_{j \in \text{nearby}(i)} p(\mathbf{T}(\mathbf{x}_i))$$

Where $\mathbf{T}$ is the current transformation and nearby($i$) returns voxels within search radius.

* Transformation Representation

Autoware uses a 6-parameter pose representation:

$$\mathbf{p} = [x, y, z, \text{roll}, \text{pitch}, \text{yaw}]^T$$

** Transformation Matrix

The pose is converted to a 4x4 homogeneous transformation matrix:

$$\mathbf{T} = \begin{bmatrix} \mathbf{R} & \mathbf{t} \\ \mathbf{0} & 1 \end{bmatrix}$$

Where $\mathbf{R}$ is the rotation matrix (X-Y-Z Euler angles) and $\mathbf{t}$ is the translation.

** Euler Angle Convention

Autoware uses X-Y-Z extrinsic rotation order:

$$\mathbf{R} = \mathbf{R}_z(\text{yaw}) \cdot \mathbf{R}_y(\text{pitch}) \cdot \mathbf{R}_x(\text{roll})$$

#+begin_src cpp
// Rotation matrix construction
Eigen::Matrix4d pose_to_matrix(const Vector6& p) {
    double cx = cos(p[3]), sx = sin(p[3]);  // roll
    double cy = cos(p[4]), sy = sin(p[4]);  // pitch
    double cz = cos(p[5]), sz = sin(p[5]);  // yaw

    Matrix4d T = Matrix4d::Identity();
    T(0,0) = cy*cz;
    T(0,1) = cz*sx*sy - cx*sz;
    T(0,2) = sx*sz + cx*cz*sy;
    T(1,0) = cy*sz;
    T(1,1) = cx*cz + sx*sy*sz;
    T(1,2) = cx*sy*sz - cz*sx;
    T(2,0) = -sy;
    T(2,1) = cy*sx;
    T(2,2) = cx*cy;
    T.block<3,1>(0,3) = p.head<3>();  // translation
    return T;
}
#+end_src

* Derivative Computation

The Newton optimization requires the gradient (6x1) and Hessian (6x6) of the score function.

** Angular Derivatives (Precomputed)

To avoid redundant computation, angular derivatives are precomputed once per iteration:

#+begin_src cpp
// From computeAngleDerivatives() - lines 576-650
void computeAngleDerivatives(const Vector6& p) {
    double cx = cos(p[3]), sx = sin(p[3]);
    double cy = cos(p[4]), sy = sin(p[4]);
    double cz = cos(p[5]), sz = sin(p[5]);

    // Jacobian terms (8 entries) - dR/d(roll,pitch,yaw)
    j_ang_(0,0) = -sx*sz + cx*cz*sy;    // d/droll of R[0,2]
    j_ang_(0,1) = -cx*sz - cz*sx*sy;    // d/droll of R[0,1]
    j_ang_(0,2) = cz*cy;                 // d/dpitch of R[0,0]
    // ... 5 more Jacobian terms

    // Hessian terms (15 entries) - d²R/d(pose)²
    h_ang_(0,0) = -cx*sz - cz*sx*sy;    // a2
    h_ang_(0,1) = -cz*sx - cx*sy*sz;    // a3
    h_ang_(0,2) = sx*sy*sz - cx*cz;     // b2
    // ... 12 more Hessian terms
}
#+end_src

These precomputed terms are reused for all points, avoiding $O(N)$ trigonometric evaluations.

** Per-Point Gradient

For each point $\mathbf{x}_i$ in voxel with mean $\boldsymbol{\mu}$ and inverse covariance $\Sigma^{-1}$:

$$\frac{\partial S}{\partial \mathbf{p}} = \sum_i d_1 d_2 \cdot e_i \cdot (\mathbf{x}_i - \boldsymbol{\mu})^T \Sigma^{-1} \frac{\partial \mathbf{x}_i}{\partial \mathbf{p}}$$

Where:
- $e_i = \exp\left(-\frac{d_2}{2}(\mathbf{x}_i - \boldsymbol{\mu})^T \Sigma^{-1} (\mathbf{x}_i - \boldsymbol{\mu})\right)$
- $\frac{\partial \mathbf{x}_i}{\partial \mathbf{p}}$ is the 3x6 Jacobian of the transformed point

** Per-Point Hessian

The Hessian includes both first and second-order terms:

$$\frac{\partial^2 S}{\partial \mathbf{p}^2} = \sum_i d_1 d_2 \cdot e_i \left[ -d_2 \cdot \mathbf{c}_i \mathbf{c}_i^T + \frac{\partial \mathbf{x}_i}{\partial \mathbf{p}}^T \Sigma^{-1} \frac{\partial \mathbf{x}_i}{\partial \mathbf{p}} + \mathbf{c}_i^T \frac{\partial^2 \mathbf{x}_i}{\partial \mathbf{p}^2} \right]$$

Where $\mathbf{c}_i = \frac{\partial \mathbf{x}_i}{\partial \mathbf{p}}^T \Sigma^{-1} (\mathbf{x}_i - \boldsymbol{\mu})$.

#+begin_NOTE
*Performance Impact*: Derivative computation is the most expensive operation per iteration. It's $O(N \times V)$ where $N$ is point count and $V$ is average voxels per point. Autoware uses OpenMP parallelization with per-thread accumulation.
#+end_NOTE

* Newton Optimization

Once we have the gradient $\mathbf{g}$ and Hessian $\mathbf{H}$, we solve for the Newton step:

$$\Delta \mathbf{p} = -\mathbf{H}^{-1} \mathbf{g}$$

** SVD Solution

Autoware uses SVD for robust inversion:

#+begin_src cpp
// From computeTransformation() lines 315-318
Eigen::JacobiSVD<Eigen::Matrix<double, 6, 6>> sv(
    hessian, Eigen::ComputeFullU | Eigen::ComputeFullV);
delta_p = sv.solve(-score_gradient);
#+end_src

SVD handles near-singular Hessians gracefully, preventing numerical instability.

** Direction Validation

The Newton direction must be a descent direction:

#+begin_src cpp
// Check descent condition
double d_phi_0 = -score_gradient.dot(delta_p);
if (d_phi_0 >= 0) {
    // Not descending, reverse direction
    delta_p *= -1.0;
    d_phi_0 *= -1.0;
}

// Normalize step if too large
double delta_p_norm = delta_p.norm();
if (delta_p_norm > 1.0) {
    delta_p /= delta_p_norm;
}
#+end_src

** Convergence Criteria

The algorithm converges when:

1. *Transformation epsilon*: $\|\Delta \mathbf{p}\| < \epsilon_{trans}$ (default: 0.01)
2. *Max iterations*: Iterations $\geq$ max_iterations (default: 30)

#+begin_src cpp
// Convergence check
nr_iterations_++;
if (nr_iterations_ >= max_iterations_ ||
    delta_p.norm() < transformation_epsilon_) {
    converged_ = true;
}
#+end_src

* Line Search (More-Thuente)

The More-Thuente line search finds an optimal step length $\alpha$ along the Newton direction.

** Why Line Search?

Newton's method can overshoot, especially early in optimization. Line search ensures sufficient decrease in the objective function while maintaining curvature.

** Strong Wolfe Conditions

The line search finds $\alpha$ satisfying:

1. *Armijo (sufficient decrease)*: $f(\mathbf{p} + \alpha \Delta\mathbf{p}) \leq f(\mathbf{p}) + \mu \alpha \nabla f^T \Delta\mathbf{p}$
2. *Curvature*: $|\nabla f(\mathbf{p} + \alpha \Delta\mathbf{p})^T \Delta\mathbf{p}| \leq \nu |\nabla f^T \Delta\mathbf{p}|$

Where $\mu = 10^{-4}$ and $\nu = 0.9$ (typical values).

** Algorithm Sketch

#+begin_src cpp
double computeStepLengthMT(Vector6& p, Vector6& delta_p,
                           double d_phi_0, double phi_0) {
    double alpha = step_size;  // Initial step (default: 0.1)
    double alpha_l = 0, alpha_u = step_max;

    for (int iter = 0; iter < 10; iter++) {
        // Evaluate function at trial step
        Vector6 trial_p = p + alpha * delta_p;
        double phi_a = computeScore(trial_p);
        double d_phi_a = computeDirectionalDerivative(trial_p, delta_p);

        // Check Wolfe conditions
        if (satisfies_armijo(phi_a) && satisfies_curvature(d_phi_a)) {
            return alpha;
        }

        // Update bracket and pick new trial
        updateInterval(alpha_l, alpha_u, alpha, phi_a, d_phi_a);
        alpha = selectTrialValue(alpha_l, alpha_u);
    }
    return alpha;
}
#+end_src

#+begin_WARNING
*Default Disabled*: Autoware disables line search by default (=use_line_search=false=) because the overhead often exceeds the benefit. The fixed step size of 0.1 works well in practice.
#+end_WARNING

** Trial Value Selection

The algorithm uses cubic/quadratic interpolation to pick trial step lengths:

#+begin_src cpp
// From trialValueSelectionMT() - simplified
double selectTrial(double a_l, double a_u, double a_t,
                   double f_l, double f_u, double f_t,
                   double g_l, double g_u, double g_t) {
    // Case 1: f_t > f_l -> Minimum between a_l and a_t
    //         Use cubic interpolation
    // Case 2: g_t * g_l < 0 -> Minimum between a_t and a_u
    //         Sign change indicates minimum exists
    // Case 3: |g_t| <= |g_l| -> Extrapolate beyond a_t
    // Case 4: Otherwise -> Safeguard interpolation
}
#+end_src

* Voxel Search Methods

How we find voxels for each point significantly affects accuracy and speed.

** KDTREE (Recommended)

Uses a KD-tree built on voxel centroids:

#+begin_src cpp
// Radius search for nearby voxels
std::vector<int> nearby;
std::vector<float> distances;
kdtree.radiusSearch(query_point, resolution, nearby, distances);

for (int voxel_idx : nearby) {
    // Accumulate score/gradient/Hessian from this voxel
}
#+end_src

*Complexity*: $O(\log V)$ per point

** DIRECT Methods (Legacy)

Directly compute neighboring voxel indices without KD-tree:

| Method | Description | Voxels Checked |
|--------+-------------+----------------|
| DIRECT1 | Only containing voxel | 1 |
| DIRECT7 | Face-adjacent neighbors | 7 |
| DIRECT26 | All neighbors including diagonals | 27 |

*Not recommended*: DIRECT methods can miss voxels near boundaries and provide less smooth gradients.

#+begin_NOTE
*Performance Impact*: KDTREE is slightly slower per query but provides smoother gradients and better convergence. Autoware defaults to KDTREE.
#+end_NOTE

* Scoring Metrics

Autoware computes two scoring metrics for quality assessment:

** Transform Probability (TP)

Average negative log-likelihood per point:

$$TP = \frac{1}{N} \sum_{i=1}^{N} \max_j\left(-d_1 \exp\left(-\frac{d_2}{2} \|\mathbf{x}_i - \boldsymbol{\mu}_j\|_{\Sigma_j}^2\right)\right)$$

Higher TP indicates better alignment. Typical range: 2.0-5.5.

** Nearest Voxel Transformation Likelihood (NVTL)

Uses only the *highest-scoring voxel* per point:

$$NVTL = \frac{1}{N_{corr}} \sum_{i \in \text{found}} \max_j\left(-d_1 \exp\left(-\frac{d_2}{2} \|\mathbf{x}_i - \boldsymbol{\mu}_j\|_{\Sigma_j}^2\right)\right)$$

NVTL normalizes by found correspondences, making it more robust to partial overlaps.

** Score Thresholds

Autoware uses score thresholds for validation:

#+begin_src cpp
// Reject alignment if score too low
if (nvtl < score_threshold) {
    // Don't publish this pose
    return false;
}
#+end_src

* Map Management

Autoware supports dynamic map loading for large-scale environments.

** Tile-Based Loading

Maps are divided into tiles that are loaded on demand:

#+begin_src cpp
// Check if position is outside current map
if (distance_to_map_center > map_radius - lidar_radius) {
    // Request new map tiles from map server
    requestDifferentialMap(current_position);
}
#+end_src

** Dual-NDT Architecture

To avoid blocking during map updates:

1. *Primary NDT*: Used for active alignment
2. *Secondary NDT*: Built in background with new tiles
3. *Atomic swap*: When secondary is ready, swap pointers

#+begin_src cpp
// Background thread builds new NDT
void mapUpdateThread() {
    while (running) {
        if (map_update_requested) {
            // Build secondary NDT with new tiles
            secondary_ndt_->setInputTarget(new_map);

            // Atomic swap
            std::lock_guard lock(ndt_mutex_);
            std::swap(primary_ndt_, secondary_ndt_);
        }
    }
}
#+end_src

* Performance Considerations

** Critical Path Analysis

| Operation | Time (typical) | Complexity |
|-----------+----------------+------------|
| Point transformation | ~0.1 ms | O(N) |
| Voxel search | ~0.5 ms | O(N log V) |
| Derivative accumulation | ~2.0 ms | O(N * V_avg) |
| Newton solve | ~0.05 ms | O(1) - 6x6 |
| Line search (if enabled) | ~2.0 ms | O(iterations * above) |

*Dominant cost*: Derivative computation, especially the nested loops for gradient/Hessian accumulation.

** OpenMP Parallelization

Autoware uses OpenMP to parallelize the derivative loop:

#+begin_src cpp
#pragma omp parallel for num_threads(threads) schedule(guided, 8) \
    reduction(+:score)
for (int i = 0; i < num_points; i++) {
    // Per-thread gradient/Hessian accumulation
    Eigen::Matrix<double, 6, 1> local_gradient;
    Eigen::Matrix<double, 6, 6> local_hessian;

    computePointDerivatives(points[i], local_gradient, local_hessian);

    // Critical section for global accumulation
    #pragma omp critical
    {
        gradient += local_gradient;
        hessian += local_hessian;
    }
}
#+end_src

#+begin_TIP
*Optimization*: Using thread-local accumulators with reduction eliminates critical section overhead. Each thread sums locally, then a final reduction combines results.
#+end_TIP

** Memory Access Patterns

For cache efficiency:
- Points stored contiguously in memory
- Voxel data (mean, icov) stored in arrays
- KD-tree queries access voxels in spatial locality

** Precomputation

| What | When Computed | Benefit |
|------+---------------+--------|
| Inverse covariance | Map load | Avoid per-point matrix inversion |
| Angular derivatives | Per iteration | Reuse across all points |
| KD-tree | Map load | Fast spatial queries |

* ROS Integration

** Subscriptions

| Topic | Type | Purpose |
|-------+------+---------|
| =/ekf_pose_with_covariance= | PoseWithCovarianceStamped | Initial guess from EKF |
| =/points_raw= | PointCloud2 | LiDAR scan |
| =/regularization_pose= | PoseWithCovarianceStamped | GNSS for regularization |

** Publications

| Topic                       | Type                      | Purpose                    |
|-----------------------------+---------------------------+----------------------------|
| =/ndt_pose=                 | PoseStamped               | Estimated pose             |
| =/ndt_pose_with_covariance= | PoseWithCovarianceStamped | Pose with uncertainty      |
| =/tf=                       | TFMessage                 | map -> base_link transform |
| =/diagnostics=              | DiagnosticArray           | Score, iterations, timing  |

** Processing Flow

#+begin_src mermaid
sequenceDiagram
    participant EKF
    participant NDT
    participant MapServer

    EKF->>NDT: Initial pose guess
    Note over NDT: Wait for scan
    LiDAR->>NDT: Point cloud
    NDT->>MapServer: Request map (if needed)
    MapServer-->>NDT: Map tiles
    Note over NDT: NDT alignment
    NDT->>EKF: Aligned pose
    NDT->>TF: Broadcast transform
#+end_src

* Accuracy Analysis: What Affects Alignment Quality?

Understanding which steps affect accuracy is critical for tuning and debugging NDT.

** Accuracy Impact Summary

| Step | Accuracy Impact | Why It Matters |
|------+-----------------|----------------|
| Voxel resolution | *Critical* | Determines spatial granularity of map representation |
| Covariance regularization | *High* | Prevents degenerate voxels, affects gradient smoothness |
| Search method (KDTREE vs DIRECT) | *High* | KDTREE provides smoother gradients, better convergence |
| Number of voxels per point | *High* | More voxels = smoother gradient = better convergence |
| Initial pose quality | *High* | Poor initialization can lead to local minima |
| Convergence threshold | *Medium* | Too loose = inaccurate; too tight = slow/non-convergent |
| Hessian regularization | *Medium* | Affects step direction in ill-conditioned regions |
| Line search | *Low-Medium* | Disabled by default; helps in difficult cases |
| Outlier ratio | *Low* | Controls Gaussian shape; 0.55 works well |

** Voxel Resolution (Critical)

The voxel resolution (default: 2.0m) is the most impactful parameter:

| Resolution | Pros | Cons | Use Case |
|------------+------+------+----------|
| 0.5-1.0m | High precision | Sensitive to noise, slow | Indoor, structured |
| 1.0-2.0m | Good balance | Standard choice | Urban driving |
| 2.0-4.0m | Fast, robust | Lower precision | Highway, open areas |

#+begin_IMPORTANT
*Rule of thumb*: Resolution should be 2-4x the expected position uncertainty. For autonomous driving with good IMU/odometry, 2.0m works well.
#+end_IMPORTANT

** Voxel Statistics Quality

Each voxel needs sufficient points for a reliable Gaussian estimate:

$$\text{min\_points\_per\_voxel} \geq 6$$

Sparse voxels (fewer points) produce unreliable covariances:

#+begin_src cpp
// Autoware default
if (voxel.nr_points < min_points_per_voxel_) {
    // Skip this voxel - not enough data for reliable statistics
    continue;
}
#+end_src

** Multi-Voxel Correspondence

The number of voxels found per point directly affects gradient quality:

| Voxels/Point | Gradient Quality | Convergence |
|--------------+------------------+-------------|
| < 1.0 | Poor (many points unmatched) | Unreliable |
| 1.0 - 2.0 | Acceptable | Usually works |
| 2.0 - 4.0 | Good (smooth gradient) | Reliable |
| > 4.0 | Excellent but slower | Best |

#+begin_NOTE
*Diagnostic*: Monitor the "voxels per point" metric. If consistently < 1.5, the map may have coverage gaps or resolution mismatch.
#+end_NOTE

** Initial Pose Quality

NDT is a local optimizer - it finds the nearest local minimum:

#+begin_src mermaid
graph LR
    A[Good Init] --> B[Global Minimum]
    C[Poor Init] --> D[Local Minimum]
    E[Very Poor Init] --> F[Divergence]
#+end_src

| Initial Error | Typical Outcome |
|---------------+-----------------|
| < 0.5m, < 5° | Converges to global minimum |
| 0.5-2.0m, 5-15° | Usually converges, may need more iterations |
| > 2.0m, > 15° | Risk of local minimum or divergence |

** Covariance Regularization

Degenerate covariances (e.g., planar surfaces) cause numerical issues:

#+begin_src cpp
// Eigenvalue clamping prevents singularity
for (int i = 0; i < 3; i++) {
    eigenvalues[i] = max(eigenvalues[i],
                         0.01 * eigenvalues.maxCoeff());
}
#+end_src

Without regularization:
- Matrix inversion fails for planar voxels
- Gradients become unstable
- Optimization may diverge

** Convergence Threshold Tuning

| Parameter | Default | Effect of Increase | Effect of Decrease |
|-----------+---------+--------------------+--------------------|
| =transformation_epsilon= | 0.01 | Faster but less accurate | More accurate but slower |
| =max_iterations= | 30 | More time for convergence | May stop early |
| =step_size= | 0.1 | Larger steps, faster | Smaller steps, more stable |

* GPU/Parallel Optimization Opportunities

This section analyzes which NDT components can benefit from GPU acceleration, parallelization, or batching.

** Optimization Opportunity Matrix

| Step | Parallelizable? | GPU Benefit | Batching Benefit | Current Status |
|------+-----------------+-------------+------------------+----------------|
| Voxel grid build | *Yes* (embarrassingly parallel) | *High* | Medium | GPU via CubeCL |
| Point transformation | *Yes* (per-point) | *High* | High | GPU kernel |
| Voxel search | *Yes* (per-point) | *Medium* | High | GPU brute-force |
| Score computation | *Yes* (per-point) | *High* | High | GPU kernel |
| Gradient computation | *Yes* (per-point, reduce) | *High* | High | GPU kernel + CUB reduce |
| Hessian computation | *Yes* (per-point, reduce) | *High* | High | GPU kernel + CUB reduce |
| Angular derivatives | No (6 trig calls) | None | None | CPU |
| Newton solve | No (6×6 system) | Low | Medium | cuSOLVER |
| Line search | Sequential | *Low* | Medium | CPU (Phase 15 planned) |
| Convergence check | No | None | None | CPU |

** Voxel Grid Construction

*Highly parallelizable* - Each point is processed independently.

#+begin_src mermaid
flowchart LR
    subgraph "GPU Pipeline (Zero-Copy)"
        A[Points N×3] --> B[Morton Codes]
        B --> C[Radix Sort]
        C --> D[Segment Detect]
        D --> E[Statistics]
    end
    E --> F[Download Results]
#+end_src

| Operation | CPU Time | GPU Time | Speedup |
|-----------+----------+----------+---------|
| Morton codes | ~50ms | ~1ms | 50× |
| Radix sort | ~100ms | ~2ms | 50× |
| Statistics | ~50ms | ~5ms | 10× |
| *Total* | ~200ms | ~15ms | ~13× |

*Key insight*: Voxel grid is built once per map load, so GPU speedup amortizes over many alignments.

** Derivative Computation (Main Bottleneck)

This is where GPU acceleration provides the most benefit:

#+begin_src cpp
// CPU: Sequential with OpenMP
for (int i = 0; i < N; i++) {           // N points
    for (int j : nearby_voxels[i]) {     // V_avg voxels per point
        accumulate_gradient(i, j);        // 6 operations
        accumulate_hessian(i, j);         // 21 operations (upper triangle)
    }
}
// Total: O(N × V_avg × 27) operations per iteration
#+end_src

*GPU approach* - Massive parallelism:

#+begin_src cpp
// GPU: One thread per point
__global__ void compute_derivatives(Points points, Voxels voxels,
                                    float* gradients, float* hessians) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i >= N) return;

    float local_grad[6] = {0};
    float local_hess[21] = {0};

    // Each thread processes all voxels for its point
    for (int j = 0; j < num_voxels; j++) {
        if (distance(points[i], voxels[j].mean) < radius) {
            accumulate(local_grad, local_hess, points[i], voxels[j]);
        }
    }

    // Write to global memory (later reduced)
    store(gradients + i*6, local_grad);
    store(hessians + i*21, local_hess);
}
#+end_src

| Metric | CPU (OpenMP 8 threads) | GPU | Speedup |
|--------+------------------------+-----+---------|
| Points | 1000 | 1000 | - |
| Voxels/point | 2.5 | 2.5 | - |
| Time/iteration | ~2.0ms | ~0.5ms | 4× |

** Reduction Strategy

The gradient (6 floats) and Hessian (21 floats) must be summed across all points:

| Strategy | Transfer Size | Performance |
|----------+---------------+-------------|
| CPU reduction | N×27 floats/iter | Bandwidth limited |
| GPU atomic add | 27 floats/iter | Contention limited |
| *GPU segmented reduce* | 27 floats/iter | Optimal |

CUB DeviceSegmentedReduce is optimal:

#+begin_src cpp
// 43 segments: 1 score + 6 gradient + 36 Hessian
cub::DeviceSegmentedReduce::Sum(
    d_temp, temp_bytes,
    d_per_point_values,   // N × 43 floats input
    d_reduced_values,     // 43 floats output
    43,                   // Number of segments
    d_segment_offsets,    // [0, N, 2N, ..., 43N]
    stream
);
#+end_src

** Newton Solve (6×6 System)

Too small for GPU parallelism to help:

| Method | Time | Notes |
|--------+------+-------|
| CPU Eigen SVD | ~50μs | Robust, handles singularity |
| CPU Eigen LLT | ~10μs | Fast, requires positive definite |
| GPU cuSOLVER | ~20μs | Overhead dominates for 6×6 |

*However*, keeping Newton solve on GPU avoids data transfer:

#+begin_src cpp
// Phase 14: Full GPU pipeline
// No CPU-GPU transfer during iteration loop
while (!converged) {
    compute_derivatives_gpu();  // GPU
    reduce_gpu();               // GPU (CUB)
    solve_newton_gpu();         // GPU (cuSOLVER)
    update_pose_gpu();          // GPU
    check_convergence_gpu();    // GPU
}
// Only download final pose
#+end_src

** Batching Opportunities

*** Multi-NDT Covariance

Evaluate M initial poses in parallel:

#+begin_src cpp
// Instead of M sequential alignments:
for (int m = 0; m < M; m++) {
    results[m] = align(points, initial_poses[m]);  // Sequential
}

// Batch on GPU:
batch_align_gpu(points, initial_poses, M);  // Parallel
#+end_src

| M (poses) | Sequential | Batched GPU | Speedup |
|-----------+------------+-------------+---------|
| 9 | 27ms | 5ms | 5.4× |
| 25 | 75ms | 12ms | 6.3× |

*** Line Search Candidates

Phase 15 design: Evaluate K step sizes in parallel:

#+begin_src cpp
// Traditional More-Thuente: Sequential trials
for (int k = 0; k < max_trials; k++) {
    alpha = try_step(alpha);  // One at a time
}

// GPU batched: Evaluate K candidates simultaneously
float alphas[K] = generate_candidates();
evaluate_all_gpu(alphas, scores, derivatives);  // Parallel
select_best(alphas, scores, derivatives);
#+end_src

** Memory Transfer Analysis

Per-iteration transfer overhead (before Phase 14):

| Direction | Data | Size (N=1000) | Time |
|-----------+------+---------------+------|
| CPU→GPU | Pose | 24 bytes | <1μs |
| CPU→GPU | Jacobians | 72 KB | ~50μs |
| CPU→GPU | Point Hessians | 576 KB | ~200μs |
| GPU→CPU | Results | 172 bytes | <1μs |
| *Total* | | ~650 KB | ~250μs |

*Phase 14 achievement*: Zero per-iteration transfers

| Phase | Per-Iteration Transfer | Total (10 iter) |
|-------+------------------------+-----------------|
| Before Phase 14 | 650 KB | 6.5 MB |
| After Phase 14 | 0 bytes | 0 bytes |

** GPU Pipeline Architecture

#+begin_src mermaid
flowchart TB
    subgraph "Per-Alignment Setup (Once)"
        A[Upload Points N×3]
        B[Upload Voxels V×12]
    end

    subgraph "Per-Iteration (All GPU)"
        C[Transform Points] --> D[Radius Search]
        D --> E[Compute Derivatives]
        E --> F[CUB Reduce]
        F --> G[cuSOLVER Newton]
        G --> H{Converged?}
        H -->|No| C
    end

    A --> C
    B --> D
    H -->|Yes| I[Download Final Pose]
#+end_src

* Current Performance Statistics

Real-world profiling data from Autoware sample rosbag (~23 seconds of driving).

** Test Environment

| Parameter | Value |
|-----------+-------|
| Date | 2026-01-10 |
| GPU | NVIDIA CUDA-capable |
| Dataset | Autoware sample rosbag |
| Map | sample-map-rosbag (point cloud) |
| Scan points | ~500-2000 per frame |
| Voxel resolution | 2.0m |

** Execution Time Comparison

| Metric | Autoware (OpenMP) | CUDA GPU | Ratio |
|--------+-------------------+----------+-------|
| *Mean* | *2.48 ms* | *13.07 ms* | *5.3× slower* |
| Median | 2.42 ms | 15.86 ms | 6.5× slower |
| Stdev | 1.20 ms | 8.24 ms | Higher variance |
| Min | 1.08 ms | 2.00 ms | 1.9× slower |
| Max | 16.46 ms | 27.02 ms | 1.6× slower |
| P95 | 3.57 ms | 24.46 ms | 6.9× slower |
| P99 | 3.98 ms | 26.34 ms | 6.6× slower |

#+begin_CAUTION
The CUDA implementation is currently slower due to convergence issues, not inherent GPU overhead. When converging quickly, CUDA achieves 2-3ms (comparable to Autoware).
#+end_CAUTION

** Convergence Statistics

| Metric | Autoware | CUDA |
|--------+----------+------|
| Convergence rate | ~100% | 51.9% |
| Mean iterations | 3-5 | 15.2 |
| Median iterations | 3-4 | 8 |
| Max iterations | ~10 | 30 (limit) |
| Hit max iterations | ~0% | 48.1% |

** Execution Time Distribution (CUDA)

| Category | Count | Percentage | Cause |
|----------+-------+------------+-------|
| Fast (<5ms) | 66 | 29.3% | Converged quickly (1-3 iter) |
| Medium (5-15ms) | 37 | 16.4% | Converged with effort |
| Slow (15-25ms) | 117 | 52.0% | Hit max iterations (30) |
| Very slow (≥25ms) | 5 | 2.2% | Worst cases |

#+begin_src mermaid
pie title CUDA NDT Execution Time Distribution
    "Fast <5ms" : 29.3
    "Medium 5-15ms" : 16.4
    "Slow 15-25ms" : 52.0
    "Very slow ≥25ms" : 2.2
#+end_src

** Score Quality Metrics

| Metric | Value |
|--------+-------|
| Mean correspondences | 3,447 points |
| Mean score | 5,363 |
| Mean NVTL | 2.16 |
| Convergence status | 56.8% Converged, 43.2% MaxIterations |

** Timing Breakdown (Per Iteration)

| Component | Autoware (CPU) | CUDA (GPU) | Notes |
|-----------+----------------+------------+-------|
| Point transformation | ~0.05ms | <0.01ms | GPU excels |
| Voxel search | ~0.3ms | ~0.1ms | GPU brute-force |
| Derivative computation | ~1.5ms | ~0.3ms | GPU kernels |
| Reduction | ~0.1ms | ~0.05ms | CUB segmented reduce |
| Newton solve | ~0.05ms | ~0.02ms | cuSOLVER |
| *Per-iteration total* | ~2.0ms | ~0.5ms | 4× faster |

#+begin_IMPORTANT
*Key insight*: Per-iteration GPU is 4× faster than CPU. The overall slowdown comes from needing 3-4× more iterations to converge (or not converging at all).
#+end_IMPORTANT

** Root Cause Analysis

*** Why Low Convergence?

The 48% non-convergence rate is the primary performance issue. Possible causes:

1. *Derivative computation differences* - Subtle numerical differences accumulate
2. *Step size behavior* - Different clamping or normalization
3. *Voxel search differences* - GPU brute-force vs CPU KD-tree
4. *Initial pose quality* - May differ due to timing

*** Trajectory Analysis

| Trajectory Segment | Entries | Behavior |
|--------------------+---------+----------|
| Start (0-86) | 86 | Fast (2-3ms), good convergence |
| Middle (87-180) | 94 | Mixed, degrading |
| End (181-225) | 45 | Mostly slow, poor convergence |

*Pattern*: Performance degrades through trajectory, suggesting:
- Map tile transitions
- Entering challenging areas (less distinctive features)
- Initial pose drift accumulation

** Comparison: Convergent vs Non-Convergent

| Metric | Converged | Max Iterations |
|--------+-----------+----------------|
| Mean time | 4.2ms | 24.1ms |
| Mean iterations | 5.3 | 30.0 |
| Mean score | 5,891 | 4,835 |
| Mean NVTL | 2.34 | 1.98 |

Non-convergent alignments have lower scores, indicating genuine matching difficulty.

** Expected vs Actual Performance

| Scenario | Expected | Actual | Gap Cause |
|----------+----------+--------+-----------|
| Fast convergence (3 iter) | 1.5ms | 2-3ms | GPU overhead |
| Normal convergence (8 iter) | 4ms | 5-8ms | Reasonable |
| Slow convergence (15 iter) | 7.5ms | 12-15ms | Reasonable |
| No convergence (30 iter) | 15ms | 20-27ms | Max iterations |

#+begin_TIP
*Optimization priority*: Focus on improving convergence rate. Even a 50%→90% convergence improvement would reduce mean time from 13ms to ~5ms.
#+end_TIP

** Phase 14 Impact (Full GPU Newton)

Before and after Phase 14 implementation:

| Metric | Before Phase 14 | After Phase 14 | Improvement |
|--------+-----------------+----------------+-------------|
| Per-iter transfer | 650 KB | 0 bytes | 100% reduction |
| Per-iter latency | ~0.8ms | ~0.5ms | 37% faster |
| Memory bandwidth | 6.5 MB/align | ~60 KB/align | 99% reduction |

*Remaining bottleneck*: Convergence rate, not GPU performance.

* Summary

** Key Takeaways

1. *NDT models the map as Gaussians* - Smooth, differentiable objective function
2. *Newton optimization* - Fast convergence with proper initialization
3. *Derivative computation is the bottleneck* - $O(N \times V_{avg})$ per iteration
4. *Precomputation is crucial* - Angular derivatives, inverse covariances, KD-tree
5. *Line search typically disabled* - Fixed step works well in practice
6. *KDTREE search recommended* - Smoother gradients than DIRECT methods

** Typical Performance

| Metric                 |    Value |
|------------------------+----------|
| Points per scan        | 500-2000 |
| Voxels per point       |      1-5 |
| Iterations to converge |     3-10 |
| Time per alignment     |   1-5 ms |

** Further Reading

- [[https://www.mrpt.org/downloads/dox/tutorial_ndt_3d.pdf][Magnusson 2009 PhD Thesis]] - Original NDT algorithm
- [[https://github.com/autowarefoundation/autoware_core][Autoware Core]] - Reference implementation
- [[https://pointclouds.org/documentation/classpcl_1_1_normal_distributions_transform.html][PCL NDT Documentation]] - PCL implementation details

* Appendix: Code References

** Autoware Files

| File                                | Description                         |
|-------------------------------------+-------------------------------------|
| =ndt_scan_matcher_core.cpp=         | ROS node, subscriptions, publishers |
| =multigrid_ndt_omp_impl.hpp=        | Core NDT algorithm                  |
| =multi_voxel_grid_covariance_omp.h= | Voxel grid data structure           |
| =map_update_module.hpp=             | Dynamic map loading                 |
| =hyper_parameters.hpp=              | Configuration parameters            |

** Key Functions

| Function                    | Location                       | Purpose                      |
|-----------------------------+--------------------------------+------------------------------|
| =computeTransformation()=   | multigrid_ndt_omp_impl.hpp:247 | Main optimization loop       |
| =computeDerivatives()=      | multigrid_ndt_omp_impl.hpp:418 | Gradient/Hessian computation |
| =computeAngleDerivatives()= | multigrid_ndt_omp_impl.hpp:576 | Precompute angular terms     |
| =computeStepLengthMT()=     | multigrid_ndt_omp_impl.hpp:971 | More-Thuente line search     |
| =updateDerivatives()=       | multigrid_ndt_omp_impl.hpp:702 | Per-point accumulation       |
